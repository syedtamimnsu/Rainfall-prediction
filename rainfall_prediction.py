# -*- coding: utf-8 -*-
"""Rainfall Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L3wGEBzqHZ5RluGERBXCfzNZGRwdSp_h

import dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import resample
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import pickle

#loading data set to pandas data frame
data = pd.read_csv('/content/Rainfall.csv')

print(type(data))

data.shape

data.head()

#show unique values of day column
data['day'].unique()

data.info()

#checks columns
data.columns

#remove extra space of all columns
data.columns = data.columns.str.strip()

#checks columns
data.columns

#drop 'day' column
data = data.drop(columns=['day'])

data.head()

#check null values
data.isnull().sum()

#handle missing values
data['winddirection'] = data['winddirection'].fillna(data['winddirection'].mode()[0])
data['windspeed'] = data['windspeed'].fillna(data['windspeed'].median())

#check null values
data.isnull().sum()

#unique values of rainfall column
data['rainfall'].unique()

#conver catagorical value to neumerical value in rainfall column
# yes --> 1    No --> 0
data['rainfall'] = data['rainfall'].map({'yes':1, 'no':0})

data.head()

"""Exploratory Data Analysis (EDA)"""

data.describe()

#setting plot style for all plots
sns.set(style='whitegrid')

data.columns

plt.figure(figsize=(15, 10))

for i, column in enumerate(['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', 'humidity','cloud', 'sunshine', 'windspeed'], 1):
  plt.subplot(3, 3, i)
  sns.histplot(data[column], kde=True)
  plt.title(f"Distribution of {column}")

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x='rainfall', data=data)
plt.title('Distribution of Rainfall')
plt.show()

#Correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("correlation heatmap")
plt.show()

#drop highly correlated column
data = data.drop(columns=['maxtemp', 'temparature', 'mintemp'] )

data.head()

"""Convert Unvalance dataset to valance dataset"""

data['rainfall'].value_counts()

#separate majority and minority class
df_majority = data[data['rainfall'] == 1]
df_minority = data[data['rainfall'] == 0]

df_majority.shape

df_minority.shape

#downsample majority class to match minority count
df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)

df_majority_downsampled.shape

#concate df_minorit and df_majority_downsampled
df_downsampled = pd.concat([df_majority_downsampled, df_minority])

df_downsampled.shape

df_downsampled.head()

#shuffle the final dataframe
df_downsampled = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)

df_downsampled.head()

df_downsampled['rainfall'].value_counts()

#split features and traget
x = df_downsampled.drop(columns=["rainfall"], axis=1)
y = df_downsampled['rainfall']

print(x)

print(y)

# splitting the data into training data and test data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)

"""Model Training"""

model_rf = RandomForestClassifier(random_state=42)

param_grid_rf = {
    "n_estimators": [50, 100, 200],
    "max_features": ["sqrt", "log2"],
    "max_depth": [None, 5, 10],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}

#Hypertuning using GridsearchCV
grid_search_rf = GridSearchCV(estimator=model_rf, param_grid=param_grid_rf, cv=5, n_jobs=1, verbose=2)
grid_search_rf.fit(x_train, y_train)

best_rf_model = grid_search_rf.best_estimator_
print("Best parameters for random forest", grid_search_rf.best_params_)

"""Model Evaluation"""

cv_score = cross_val_score(best_rf_model, x_train, y_train, cv=5)
print("Cross validation score: ", cv_score)
print("mean cross validation score: ",np.mean(cv_score) )

#test set performance
y_pred = best_rf_model.predict(x_test)

print("Test set accuracy: ", accuracy_score(y_test, y_pred))
print("Test set confusion Matrix: \n", confusion_matrix(y_test, y_pred))
print("Classification report:\n", classification_report(y_test, y_pred))

"""prediction on uknown data"""

input_data = (1015.9, 19.9, 95, 81, 0.0, 40.0, 13.7)

input_df = pd.DataFrame([input_data], columns=['pressure', 'dewpoint', 'humidity', 'cloud', 'sunshine','winddirection', 'windspeed'])

input_df

prediction = best_rf_model.predict(input_df)

print(prediction)

prediction = best_rf_model.predict(input_df)
print("Prediction result: ", "Rainfall" if prediction[0]==1 else "No Rainfall" )

"""save model and feature names to a pickle file"""

#save model and feature names to a pickle file
model_data = {"model": best_rf_model, "feature_names": x.columns.tolist()}

# Save the model to a file using pickle.dump
with open("rainfall_prediction_model.pkl", "wb") as file: # Changed mode to 'wb' for writing binary
  pickle.dump(model_data, file)

# The following line is for loading the model, it should be used later if you want to load the model
# with open("rainfall_prediction_model.pkl", "rb") as file:
#   model_data = pickle.load(file)

model = model_data["model"]
feature_names = model_data["feature_names"]

input_data = (1015.9, 19.9, 95, 81, 0.0, 40.0, 13.7)

input_df = pd.DataFrame([input_data], columns=feature_names)

prediction = best_rf_model.predict(input_df)
print("Prediction result:", "Rainfall" if prediction[0] == 1 else "No Rainfall")